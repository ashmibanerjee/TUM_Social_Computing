{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Computing - Summer 2018\n",
    "# Exercise 4 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4.1. K-means Clustering of social network data\n",
    "\n",
    "Write a Python program that computes K-means clustering for a given social network dataset. The input dataset (file: networkinput.csv) contains anyonymized data from user profiles of a small social networking platform. Each line in the file represents one feature vector and is associated with a social network user. The vectors have a name (\"userXYZ\") and the following 4 features:\n",
    "* Number of posts\n",
    "* Number of comments\n",
    "* Number of likes (on both posts and comments)\n",
    "* Number of friends and followers (average)\n",
    "\n",
    "By clustering the users, you identify users with similar activity patterns. This can be helpful for research but also for advertisers and polling firms.\n",
    "\n",
    "Your program should compute K-means clustering for the dataset according to the formula discussed in the lecture ==> Minimizing the objective function:\n",
    "$$\\sum_{k=1}^{K} \\sum_{\\{n|x_n \\in C_k \\}} \\|x_n - \\mu_k\\|^2$$\n",
    "Such that:\n",
    "$K$ is the number of clusters, $x_n$ is the nth point that belongs to the $k$th cluster, and $\\mu_k$ is the centroid (prototype) of the $k$th cluster. (Refer to the lecture for details)\n",
    "The K-means clustering algorithm should proceed as follows:\n",
    "1. The program should start by parsing the dataset \n",
    "2. Assign four random centroids (prototypes).\n",
    "3. Assign data points to the nearest centroid. \n",
    "4. Recompute the centroid values: The new centroid values of the kth centroid are calculated as the average values of the points currently in that centroid.\n",
    "5. Repeat from point 3 till the values of the centroids don't change anymore.\n",
    "\n",
    "<b>The output of your program should be a a list that asigns a cluster ID (0, 1, 2, 3) to every user in the input file. </b>\n",
    "The first argument in that tuple should be the users's name (e.g., \"user111\") and the second argument should be the centroid id to which this user is associated to. e.g ('user111', 3).\n",
    "**Note:** this output value should be the final centroid values that don't change anymore.\n",
    "\n",
    "\n",
    "<b> After the clustering with k=4 is complete, run the code with the following centroid starting points:\n",
    "\n",
    "centroids = {0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]}.\n",
    "\n",
    "Have a look at the result and describe the common properties in each of the four groups</b> (max 5 sentences).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### RUN THIS ON PYTHON 2.0 ##\n",
    "import csv \n",
    "import random \n",
    "import numpy as np \n",
    "from scipy.spatial import distance \n",
    "import collections\n",
    "\n",
    "# TODO: Read the network activity data set into a dictionary (observations_dictionary)\n",
    "def read_data_set(dataset_path):\n",
    "    observations = {}\n",
    "    for line in open(dataset_path):\n",
    "            row = line.strip().split(',')\n",
    "            try:\n",
    "                user_id, posts, comments, likes, followers = row[0], int(row[1]), int(row[2]), int(row[3]), int(row[4])\n",
    "                observations[user_id] = posts, comments, likes, followers\n",
    "            except KeyError:\n",
    "                print(\"key error found! \")\n",
    "    return observations\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Assign random centroids\n",
    "# Hint: centroid values should be randomly assigned so that for each dimension of the centroid, the random values should fall\n",
    "# between the maximum and the minimum value of that dimension of all the points in the data set.\n",
    "# For example: for the dimension \"Number of Comments\", if the maximum value = 49, and the minimum value = 0, the value assigned to\n",
    "# the dimension \"Murder\" of the centroid should be randomly assigned between 0 and 49\n",
    "\n",
    "def create_random_centroid_values(observations, k): #, centroid):\n",
    "    # find min and max of observations in each dimension\n",
    "    # create dictionary with four random centroids within the observed space\n",
    "\n",
    "    dimension_arr = np.array([])  # Initializing an empty numpy array\n",
    "    for user, cluster_id in observations.items():\n",
    "        if dimension_arr.size == 0:  ## when array is empty\n",
    "            dimension_arr = np.asarray(list(cluster_id))  # First iteration, create the array\n",
    "        else:\n",
    "            dimension_arr = np.vstack((dimension_arr, cluster_id))\n",
    "\n",
    "    minarr = np.amin(dimension_arr, axis=0)\n",
    "    maxarr = np.amax(dimension_arr, axis=0)\n",
    "\n",
    "    i = 0\n",
    "    centroids = {}\n",
    "    while (i < k):\n",
    "        centroids[i] = [np.random.uniform(minarr[0], maxarr[0]), np.random.uniform(minarr[1], maxarr[1]),\n",
    "                        np.random.uniform(minarr[2], maxarr[2]), np.random.uniform(minarr[3], maxarr[3])]\n",
    "        i += 1\n",
    "\n",
    "    return centroids\n",
    "    \n",
    "    \n",
    "# Assign centroid ID for each of the data points. Each data item is assigned to its closest centroid\n",
    "def update_observation_centroids(observations, centroids, k, observation_centroids=None):\n",
    "    # create dictionary mapping each observation to a centroid index\n",
    "    if centroids == None:\n",
    "        # # initial run: For each centroid i of the k centroids, create random values\n",
    "        centroids = create_random_centroid_values(observations, k)\n",
    "    # Otherwise\n",
    "    observation_centroids = observations.copy()\n",
    "    centroid_id = 5\n",
    "    ## TODO: might have to change\n",
    "    mindist = float(\"inf\")\n",
    "    for key in observations:\n",
    "          # TODO: Each centroid i of the K centroids is the average of the data values previously assigned to that centroid i\n",
    "        for c in centroids:\n",
    "            if distance.euclidean(observations[key], centroids[c]) < mindist:\n",
    "                mindist = distance.euclidean(observations[key], centroids[c])\n",
    "                centroid_id = c\n",
    "        observation_centroids[key] = centroid_id\n",
    "    # print(\"Observation Centroids: \\n\",observation_centroids)\n",
    "    return observation_centroids # TODO: Return the newly created observation centroids\n",
    "\n",
    "\n",
    "# Create new centroid values for each cluster as the mean of data values for the points in that cluster\n",
    "def update_centroid_values(observations, observation_centroids, k):\n",
    "    updated_centroids = {}\n",
    "    posts = [[0],[0],[0],[0]]\n",
    "    comments = [[0],[0],[0],[0]]\n",
    "    likes = [[0],[0],[0],[0]]\n",
    "    followers = [[0],[0],[0],[0]]\n",
    "    for user in observations:\n",
    "        i = observation_centroids[user]\n",
    "        posts[i].append(observations[user][0])\n",
    "        comments[i].append(observations[user][1])\n",
    "        likes[i].append(observations[user][2])\n",
    "        followers[i].append(observations[user][3])\n",
    "\n",
    "    ## Converting list of list to np.array of array so that we can perform np.mean operation\n",
    "    posts = np.array([np.array(xi) for xi in posts])\n",
    "    comments = np.array([np.array(xi) for xi in comments])\n",
    "    likes = np.array([np.array(xi) for xi in likes])\n",
    "    followers = np.array([np.array(xi) for xi in followers])\n",
    "\n",
    "    ## since all arrays of arrays are of different lengths hence specifying the axis in np.mean didn't work\n",
    "    posts_mean = [np.mean(i) for i in posts]\n",
    "    comments_mean = [np.mean(i) for i in comments]\n",
    "    likes_mean = [np.mean(i) for i in likes]\n",
    "    followers_mean = [np.mean(i) for i in followers]\n",
    "\n",
    "    for i in range(0,k):\n",
    "        updated_centroids[i] = [posts_mean[i], comments_mean[i], likes_mean[i], followers_mean[i]]\n",
    "#     print(\"Updated Centroids: \\n\",updated_centroids)\n",
    "\n",
    "    return updated_centroids    \n",
    "\n",
    "def stopping_criteria(centroids, new_centroids):\n",
    "    if centroids == None or new_centroids == None:\n",
    "        return False\n",
    "    # Extract the values and store them in lists\n",
    "    centroid_values = centroids.values()\n",
    "    new_centroid_values = new_centroids.values()\n",
    "\n",
    "    # Compare the 2 lists to see if they contain the same values\n",
    "    equal = True\n",
    "    for i in centroid_values:\n",
    "        equal = equal & (i in new_centroid_values)\n",
    "\n",
    "    return equal\n",
    "\n",
    "def  calculate_k_means_clustering(data_set_path, k):\n",
    "    observations = read_data_set(data_set_path)\n",
    "    centroids = {0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]}\n",
    "    new_centroids = dict.fromkeys(range(0,k),0)\n",
    "    count = 0\n",
    "    # TODO: Compare the new centroid dictionary with the old centroid dictionary\n",
    "\n",
    "    while (1):\n",
    "        centroids = new_centroids\n",
    "        # Go through all observations and assign them to the centroid closest to them.\n",
    "        observation_centroids = update_observation_centroids(observations, centroids, k)\n",
    "        # Compute updated centroids by averaging the values of the observations assigned to the centroid in question,\n",
    "        # using the dictionary calculated in the update_observation_centroids method.\n",
    "        new_centroids = update_centroid_values(observations, observation_centroids, k)\n",
    "#         print('New centroids:', new_centroids)\n",
    "#         print(\"Old centroids:\", centroids)\n",
    "        count += 1\n",
    "        if stopping_criteria(centroids, new_centroids) == True:\n",
    "            break\n",
    "    print(\"Attempt:= \", count)\n",
    "    return observation_centroids\n",
    "\n",
    "  \n",
    "        \n",
    "        \n",
    "# # run code\n",
    "# calculate_k_means_clustering('networkinput.csv', 4)\n",
    "dict_centroids = calculate_k_means_clustering('networkinput.csv', 4)\n",
    "    # Sorting the dictionary\n",
    "dict_final = collections.OrderedDict(sorted(dict_centroids.items(), key=lambda x: x[1]))\n",
    "\n",
    "print(\"\\n \\n\")\n",
    "print(dict_final)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Remark on Numerics of K-Means (also for GMMs)</h2>\n",
    "\n",
    "When using a random seed, it might happen that a cluster is finally empty. A common strategy to prevent such results is a (random) re-initialization of empty clusters. In order to get everyone on the same boat, we have also provided a given seed ([9, 33, 29, 25], [4, 44, 12, 41], [10, 13, 44, 65], [10, 44, 48, 70]). That means, everyone should be able to produce the exact same results.\n",
    "\n",
    "With this seed, you should not get any empty clusters any empty final clusters.\n",
    "\n",
    "If you want to compare with library functions, you should get the same results. Since there is a huge discussion going on, and multiple code snippets have been posted, here is a very simple code using sklearn.cluster:\n",
    "\n",
    "<code>\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy import genfromtxt\n",
    "\n",
    "observations = genfromtxt('networkinput.csv', delimiter=',')[:, 1 : 5]\n",
    "centers = np.asarray([[9, 33, 29, 25], [4, 44, 12, 41], [10, 13, 44, 65], [10, 44, 48, 70]])\n",
    "kmeans = KMeans(n_clusters=4, init=centers).fit(observations)\n",
    "result = kmeans.predict(observations)\n",
    "print sum(result == 0), sum(result == 1), sum(result == 2), sum(result == 3)\n",
    "</code>\n",
    "\n",
    "\n",
    "This does not produce any final empty clusters and it can be used as a reference point for validating your implementation.\n",
    "\n",
    "While this code does not produce any final empty clusters, sklean.cluster.KMeans obviously re-initializes an empty cluster. As already mentioned by multiple participants, an empty cluster will be created in iteration 1.\n",
    "\n",
    "One common way to fix this situation is the re-initialization of that particular cluster using a random point or points that are far away from their centroid. Setting an empty cluster to zero is usually not a good idea - it also depends on the specific underlying space, e.g. the range of our observations. For this particular exercise, setting the center to zero creates a similar effect as setting the center to points that are far away from their centroid, because 0 is located at the boundaries of our data points.\n",
    " \n",
    "You can check out the method used by sklean.cluster.KMeans here: https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cluster/_k_means.pyx.\n",
    "\n",
    "Throwing away an empty cluster is usually not a good idea, since we want to specify the amount of clusters in advance. A 4-means-clustering is supposed to create 4 final prototypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Problem 4.2. Girvan-Newman Algorithm \n",
    "\n",
    "This problem is optional. That means you do not HAVE to do it to get full marks. However, it is certainly beneficial to counterbalance potential flaws of your solution to problem 4.1. It is, of course, also beneficial if you want to learn something. \n",
    "\n",
    "The Girvan-Newman algorithm is an efficient algorithm for computing graph clustering.\n",
    "\n",
    "Write a Python program that computes the best clustering for the Krackhardt Kite graph.\n",
    "Remember from the lecture that the central idea of the Girvan-Newman algorithm is to compute the edge betweenness in the input graph. Large betweenness value is an indication that the corresponding edge is a bridge between two clusters in the graph, and cutting that edge means isolating those clusters.\n",
    "The algorithm proceeds by determining edge betweenness values for all the edges in the graph, and removing the edge with the highest betweenness value and repeating until there are no more edges.\n",
    "The output of the Girvan-Newman algorithm is a dendrogram of clusters where individual vertices are at the bottom. Therefore, it's necessary to cut that dendrogram and determine the best cluster. Best clustering is the one with the highest graph modularity value, which is determined by the formula:\n",
    "$$ Q = \\sum_{i} (e_{ii} - a_i^2) $$\n",
    "where $e_{ii}$ sums the fraction of graph edges that connects nodes in the ith cluster. And $a_i$ is the fraction of edges that connect to the ith cluster (see lecture for details)\n",
    "The input to the Python program will be the Krackhardt Kite Graph as an igraph Graph object. The output should be a tuple of two arguments:\n",
    "The first should be the value of the best modularity corresponding to the best graph clustering (a floating point number). The second argument should be a tuple of igraph Graph objects representing the clusters. \n",
    "\n",
    "For computing the edge betweenness, you can use any APSP-algorithm (e.g. Floyd Warshall) or (better) you can use a variation of the algorithm for calculating hortest path betweenness centrality by Ulrik Brandes (U. Brandes, A faster algorithm for betweenness centrality. Journal of Mathematical Sociology 25, 163–177 (2001), that was referenced in the paper M. E. J. Newman and M. Girvan: Finding and evaluating community structure in networks. Arxiv, 2003) and that is state of the art for computing shortest path betweenness centrality.\n",
    "You may in any case use a library procedure for calculating the edge betweenness centrality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import igraph\n",
    "import Queue\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_node_degrees(g):\n",
    "    # HINT: You will need to calculate the adjacency matrix of the graph\n",
    "    # Some Graph methods from igraph could be helpful\n",
    "\n",
    "def calculate_modularity(g, original_deg_dict, m):\n",
    "    modularity = 0\n",
    "    degree_dict = calculate_node_degrees(g)\n",
    "    connected_components = g.components()\n",
    "    for i in range(len(connected_components)):\n",
    "        subgraph = connected_components.subgraph(i) # each subgraph represents a cluster in the graph\n",
    "        e = 0 # Fraction of edges that connect to cluster\n",
    "        a = 0 # Fraction of edges that connect to cluster with random connections between edges\n",
    "        for v in subgraph.vs:\n",
    "            e += degree_dict[v.index]\n",
    "            a += original_deg_dict[v.index]\n",
    "        # TODO: Calculate the modularity\n",
    "    return modularity\n",
    "        \n",
    "# TODO: Calculate edge betweeness  \n",
    "def calculate_edge_betweenness(g):\n",
    "\n",
    "def calculate_girvan_newman_clustering(g):\n",
    "    m = # TODO: The original number of edges\n",
    "    original_degree = calculate_node_degrees(g)\n",
    "    while # TODO: Graph still has edges:\n",
    "        Q = calculate_modularity(g, original_degree, m)\n",
    "        if (Q > largest_modularity):\n",
    "            largest_modulatirty = Q\n",
    "            edge_betweenness = calculate_edge_betweenness(g) # TODO: get edge with the highest betweenness value\n",
    "            # TODO: delete the edge with the maximum betweenness value\n",
    "            # Hint: Some built-in methods from igraph could be helpful\n",
    "    \n",
    "    # Finally\n",
    "    return final_graph_clustering, largest_modularity\n",
    "\n",
    "# Program's entry point:\n",
    "g = igraph.Graph.Famous('Krackhardt_Kite') # Connected, Unweighted, undirected social network\n",
    "calculate_girvan_newman_clustering(g)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
